{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to school today and I met a friend.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# ì‚¬ì „ í•™ìŠµëœ MarianMT ëª¨ë¸ (ì˜ˆì œ: í•œêµ­ì–´ â†” ì˜ì–´\n",
    "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate(text):\n",
    "    tokens = tokenizer(text, return_tensors = \"pt\", padding = True, truncation = True)\n",
    "    translated_tokens = model.generate(**tokens)\n",
    "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens = True)\n",
    "    return translated_text[0]\n",
    "\n",
    "translated_text = translate(\"ë‚˜ëŠ” ì˜¤ëŠ˜ í•™êµì— ê°€ì„œ ì¹œêµ¬ë¥¼ ë§Œë‚¬ì–´\")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to school today and met a friend.\n"
     ]
    },
    {
     "ename": "LanguageNotSupportedException",
     "evalue": "sgn-US --> No support for the provided language.\nPlease select on of the supported languages:\n{'afrikaans': 'af', 'albanian': 'sq', 'amharic': 'am', 'arabic': 'ar', 'armenian': 'hy', 'assamese': 'as', 'aymara': 'ay', 'azerbaijani': 'az', 'bambara': 'bm', 'basque': 'eu', 'belarusian': 'be', 'bengali': 'bn', 'bhojpuri': 'bho', 'bosnian': 'bs', 'bulgarian': 'bg', 'catalan': 'ca', 'cebuano': 'ceb', 'chichewa': 'ny', 'chinese (simplified)': 'zh-CN', 'chinese (traditional)': 'zh-TW', 'corsican': 'co', 'croatian': 'hr', 'czech': 'cs', 'danish': 'da', 'dhivehi': 'dv', 'dogri': 'doi', 'dutch': 'nl', 'english': 'en', 'esperanto': 'eo', 'estonian': 'et', 'ewe': 'ee', 'filipino': 'tl', 'finnish': 'fi', 'french': 'fr', 'frisian': 'fy', 'galician': 'gl', 'georgian': 'ka', 'german': 'de', 'greek': 'el', 'guarani': 'gn', 'gujarati': 'gu', 'haitian creole': 'ht', 'hausa': 'ha', 'hawaiian': 'haw', 'hebrew': 'iw', 'hindi': 'hi', 'hmong': 'hmn', 'hungarian': 'hu', 'icelandic': 'is', 'igbo': 'ig', 'ilocano': 'ilo', 'indonesian': 'id', 'irish': 'ga', 'italian': 'it', 'japanese': 'ja', 'javanese': 'jw', 'kannada': 'kn', 'kazakh': 'kk', 'khmer': 'km', 'kinyarwanda': 'rw', 'konkani': 'gom', 'korean': 'ko', 'krio': 'kri', 'kurdish (kurmanji)': 'ku', 'kurdish (sorani)': 'ckb', 'kyrgyz': 'ky', 'lao': 'lo', 'latin': 'la', 'latvian': 'lv', 'lingala': 'ln', 'lithuanian': 'lt', 'luganda': 'lg', 'luxembourgish': 'lb', 'macedonian': 'mk', 'maithili': 'mai', 'malagasy': 'mg', 'malay': 'ms', 'malayalam': 'ml', 'maltese': 'mt', 'maori': 'mi', 'marathi': 'mr', 'meiteilon (manipuri)': 'mni-Mtei', 'mizo': 'lus', 'mongolian': 'mn', 'myanmar': 'my', 'nepali': 'ne', 'norwegian': 'no', 'odia (oriya)': 'or', 'oromo': 'om', 'pashto': 'ps', 'persian': 'fa', 'polish': 'pl', 'portuguese': 'pt', 'punjabi': 'pa', 'quechua': 'qu', 'romanian': 'ro', 'russian': 'ru', 'samoan': 'sm', 'sanskrit': 'sa', 'scots gaelic': 'gd', 'sepedi': 'nso', 'serbian': 'sr', 'sesotho': 'st', 'shona': 'sn', 'sindhi': 'sd', 'sinhala': 'si', 'slovak': 'sk', 'slovenian': 'sl', 'somali': 'so', 'spanish': 'es', 'sundanese': 'su', 'swahili': 'sw', 'swedish': 'sv', 'tajik': 'tg', 'tamil': 'ta', 'tatar': 'tt', 'telugu': 'te', 'thai': 'th', 'tigrinya': 'ti', 'tsonga': 'ts', 'turkish': 'tr', 'turkmen': 'tk', 'twi': 'ak', 'ukrainian': 'uk', 'urdu': 'ur', 'uyghur': 'ug', 'uzbek': 'uz', 'vietnamese': 'vi', 'welsh': 'cy', 'xhosa': 'xh', 'yiddish': 'yi', 'yoruba': 'yo', 'zulu': 'zu'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLanguageNotSupportedException\u001b[39m             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(translated_text)  \u001b[38;5;66;03m# \"I went to school today and met my friend.\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ì˜ì–´ â†’ ë¯¸êµ­ìˆ˜ì–´(ASL) ë²ˆì—­ (Google APIì—ì„œëŠ” ì™„ì „í•œ ì§€ì›ì´ ì•ˆ ë  ìˆ˜ ìˆìŒ)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m asl_translation = \u001b[43mGoogleTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msgn-US\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.translate(translated_text)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(asl_translation)  \u001b[38;5;66;03m# ASL ìŠ¤íƒ€ì¼ ë¬¸ì¥ ì¶œë ¥ ê°€ëŠ¥ì„± ìˆìŒ\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/superbad/lib/python3.12/site-packages/deep_translator/google.py:39\u001b[39m, in \u001b[36mGoogleTranslator.__init__\u001b[39m\u001b[34m(self, source, target, proxies, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m@param source: source language to translate from\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m@param target: target language to translate to\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.proxies = proxies\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_URLS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGOOGLE_TRANSLATE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43melement_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdiv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43melement_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclass\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mt0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpayload_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mq\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# key of text in the url\u001b[39;49;00m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m._alt_element_query = {\u001b[33m\"\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mresult-container\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/superbad/lib/python3.12/site-packages/deep_translator/base.py:44\u001b[39m, in \u001b[36mBaseTranslator.__init__\u001b[39m\u001b[34m(self, base_url, languages, source, target, payload_key, element_tag, element_query, **url_params)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidSourceOrTargetLanguage(target)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28mself\u001b[39m._source, \u001b[38;5;28mself\u001b[39m._target = \u001b[38;5;28mself\u001b[39m._map_language_to_code(source, target)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m._url_params = url_params\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m._element_tag = element_tag\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/superbad/lib/python3.12/site-packages/deep_translator/base.py:84\u001b[39m, in \u001b[36mBaseTranslator._map_language_to_code\u001b[39m\u001b[34m(self, *languages)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._languages[language]\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LanguageNotSupportedException(\n\u001b[32m     85\u001b[39m         language,\n\u001b[32m     86\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo support for the provided language.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease select on of the supported languages:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._languages\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     89\u001b[39m     )\n",
      "\u001b[31mLanguageNotSupportedException\u001b[39m: sgn-US --> No support for the provided language.\nPlease select on of the supported languages:\n{'afrikaans': 'af', 'albanian': 'sq', 'amharic': 'am', 'arabic': 'ar', 'armenian': 'hy', 'assamese': 'as', 'aymara': 'ay', 'azerbaijani': 'az', 'bambara': 'bm', 'basque': 'eu', 'belarusian': 'be', 'bengali': 'bn', 'bhojpuri': 'bho', 'bosnian': 'bs', 'bulgarian': 'bg', 'catalan': 'ca', 'cebuano': 'ceb', 'chichewa': 'ny', 'chinese (simplified)': 'zh-CN', 'chinese (traditional)': 'zh-TW', 'corsican': 'co', 'croatian': 'hr', 'czech': 'cs', 'danish': 'da', 'dhivehi': 'dv', 'dogri': 'doi', 'dutch': 'nl', 'english': 'en', 'esperanto': 'eo', 'estonian': 'et', 'ewe': 'ee', 'filipino': 'tl', 'finnish': 'fi', 'french': 'fr', 'frisian': 'fy', 'galician': 'gl', 'georgian': 'ka', 'german': 'de', 'greek': 'el', 'guarani': 'gn', 'gujarati': 'gu', 'haitian creole': 'ht', 'hausa': 'ha', 'hawaiian': 'haw', 'hebrew': 'iw', 'hindi': 'hi', 'hmong': 'hmn', 'hungarian': 'hu', 'icelandic': 'is', 'igbo': 'ig', 'ilocano': 'ilo', 'indonesian': 'id', 'irish': 'ga', 'italian': 'it', 'japanese': 'ja', 'javanese': 'jw', 'kannada': 'kn', 'kazakh': 'kk', 'khmer': 'km', 'kinyarwanda': 'rw', 'konkani': 'gom', 'korean': 'ko', 'krio': 'kri', 'kurdish (kurmanji)': 'ku', 'kurdish (sorani)': 'ckb', 'kyrgyz': 'ky', 'lao': 'lo', 'latin': 'la', 'latvian': 'lv', 'lingala': 'ln', 'lithuanian': 'lt', 'luganda': 'lg', 'luxembourgish': 'lb', 'macedonian': 'mk', 'maithili': 'mai', 'malagasy': 'mg', 'malay': 'ms', 'malayalam': 'ml', 'maltese': 'mt', 'maori': 'mi', 'marathi': 'mr', 'meiteilon (manipuri)': 'mni-Mtei', 'mizo': 'lus', 'mongolian': 'mn', 'myanmar': 'my', 'nepali': 'ne', 'norwegian': 'no', 'odia (oriya)': 'or', 'oromo': 'om', 'pashto': 'ps', 'persian': 'fa', 'polish': 'pl', 'portuguese': 'pt', 'punjabi': 'pa', 'quechua': 'qu', 'romanian': 'ro', 'russian': 'ru', 'samoan': 'sm', 'sanskrit': 'sa', 'scots gaelic': 'gd', 'sepedi': 'nso', 'serbian': 'sr', 'sesotho': 'st', 'shona': 'sn', 'sindhi': 'sd', 'sinhala': 'si', 'slovak': 'sk', 'slovenian': 'sl', 'somali': 'so', 'spanish': 'es', 'sundanese': 'su', 'swahili': 'sw', 'swedish': 'sv', 'tajik': 'tg', 'tamil': 'ta', 'tatar': 'tt', 'telugu': 'te', 'thai': 'th', 'tigrinya': 'ti', 'tsonga': 'ts', 'turkish': 'tr', 'turkmen': 'tk', 'twi': 'ak', 'ukrainian': 'uk', 'urdu': 'ur', 'uyghur': 'ug', 'uzbek': 'uz', 'vietnamese': 'vi', 'welsh': 'cy', 'xhosa': 'xh', 'yiddish': 'yi', 'yoruba': 'yo', 'zulu': 'zu'}"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­\n",
    "text = \"ë‚˜ëŠ” ì˜¤ëŠ˜ í•™êµì— ê°€ì„œ ì¹œêµ¬ë¥¼ ë§Œë‚¬ì–´.\"\n",
    "translated_text = GoogleTranslator(source='ko', target='en').translate(text)\n",
    "print(translated_text)  # \"I went to school today and met my friend.\"\n",
    "\n",
    "# ì˜ì–´ â†’ ë¯¸êµ­ìˆ˜ì–´(ASL) ë²ˆì—­ (Google APIì—ì„œëŠ” ì™„ì „í•œ ì§€ì›ì´ ì•ˆ ë  ìˆ˜ ìˆìŒ)\n",
    "asl_translation = GoogleTranslator(source='en', target='sgn-US').translate(translated_text)\n",
    "print(asl_translation)  # ASL ìŠ¤íƒ€ì¼ ë¬¸ì¥ ì¶œë ¥ ê°€ëŠ¥ì„± ìˆìŒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 94.7MB/s]                    \n",
      "2025-03-14 17:46:19 INFO: Downloaded file to /home/usou/stanza_resources/resources.json\n",
      "2025-03-14 17:46:19 INFO: Downloading default packages for language: ko (Korean) ...\n",
      "2025-03-14 17:46:20 INFO: File exists: /home/usou/stanza_resources/ko/default.zip\n",
      "2025-03-14 17:46:21 INFO: Finished downloading models and saved to /home/usou/stanza_resources\n",
      "2025-03-14 17:46:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 137MB/s]                     \n",
      "2025-03-14 17:46:21 INFO: Downloaded file to /home/usou/stanza_resources/resources.json\n",
      "2025-03-14 17:46:22 INFO: Loading these models for language: ko (Korean):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | kaist          |\n",
      "| pos       | kaist_nocharlm |\n",
      "| lemma     | kaist_nocharlm |\n",
      "| depparse  | kaist_nocharlm |\n",
      "==============================\n",
      "\n",
      "2025-03-14 17:46:22 INFO: Using device: cuda\n",
      "2025-03-14 17:46:22 INFO: Loading: tokenize\n",
      "2025-03-14 17:46:22 INFO: Loading: pos\n",
      "2025-03-14 17:46:24 INFO: Loading: lemma\n",
      "2025-03-14 17:46:25 INFO: Loading: depparse\n",
      "2025-03-14 17:46:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ë¬¸ì¥: ë‚˜ëŠ” ì§€ê¸ˆ ì§‘ì— ê°€ê³  ì‹¶ì–´.\n",
      "ìˆ˜ì–´ì²´ ë³€í™˜: ë‚˜ ì§€ê¸ˆ ì§‘ ê°€ë‹¤.\n",
      "\n",
      "ì›ë³¸ ë¬¸ì¥: ë‚´ì¼ ì˜í™” ë³´ê³  ì‹¶ì–´.\n",
      "ìˆ˜ì–´ì²´ ë³€í™˜: ë‚´ì¼ ì˜í™” ë³´ë‹¤.\n",
      "\n",
      "ì›ë³¸ ë¬¸ì¥: ì €ëŠ” ì§‘ì—ì„œ ë°¥ì„ ë¨¹ê³  ì‹¶ì–´ìš”.\n",
      "ìˆ˜ì–´ì²´ ë³€í™˜: ì € ì§‘ ë°¥ ë¨¹ë‹¤.\n",
      "\n",
      "ì›ë³¸ ë¬¸ì¥: ê³ ì–‘ì´ê°€ ì°½ë¬¸ ìœ„ì—ì„œ ìê³  ìˆì–´.\n",
      "ìˆ˜ì–´ì²´ ë³€í™˜: ê³ ì–‘ì´ ì°½ë¬¸ ìœ„ ìë‹¤.\n",
      "\n",
      "ì›ë³¸ ë¬¸ì¥: ë‚˜ëŠ” í•œêµ­ì–´ë¥¼ ë°°ìš°ê³  ì‹¶ì§€ ì•Šì•„.\n",
      "ìˆ˜ì–´ì²´ ë³€í™˜: ë‚˜ í•œêµ­ì–´ ë°°ìš°ë‹¤.\n",
      "\n",
      "ì›ë³¸ ë¬¸ì¥: ë‚˜ëŠ” ìƒˆë¡œìš´ í•¸ë“œí°ì„ ì‚¬ê³  ì‹¶ì–´.\n",
      "ìˆ˜ì–´ì²´ ë³€í™˜: ë‚˜ ìƒˆë¡œìš´ í•¸ë“œí° ì‚¬ë‹¤.\n",
      "\n",
      "ì›ë³¸ ë¬¸ì¥: ê·¸ëŠ” ì±…ì„ ì½ê³  ìˆì–´.\n",
      "ìˆ˜ì–´ì²´ ë³€í™˜: ê·¸ ì±… ì½ë‹¤.\n",
      "\n",
      "ì›ë³¸ ë¬¸ì¥: ë¬¸ë¬¸ì€ ë°¥ì„ ë¨¹ê³  ìˆì–´.\n",
      "ìˆ˜ì–´ì²´ ë³€í™˜: ë¬¸ë¬¸ ë°¥ ë¨¹ë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import re\n",
    "\n",
    "# Stanza í•œêµ­ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ í•„ìš”)\n",
    "stanza.download(\"ko\")\n",
    "nlp = stanza.Pipeline(\"ko\", processors=\"tokenize,pos,lemma,depparse\")\n",
    "\n",
    "# âœ… ëª¨ë“  ì¡°ì‚¬(ì€, ëŠ”, ì´, ê°€, ë¥¼, ì„ ë“±)ë¥¼ ìë™ ì œê±°í•˜ëŠ” í•¨ìˆ˜\n",
    "def remove_josa(word):\n",
    "    \"\"\"\n",
    "    í•œêµ­ì–´ì—ì„œ ëª…ì‚¬(NOUN) ë’¤ì— ë¶™ì€ ì¡°ì‚¬(ì€, ëŠ”, ì´, ê°€, ë¥¼, ì„ ë“±)ë¥¼ ìë™ ì œê±°\n",
    "    \"\"\"\n",
    "    return re.sub(r\"(ì€|ëŠ”|ì´|ê°€|ë¥¼|ì„)$\", \"\", word)\n",
    "\n",
    "# âœ… ë™ì‚¬ ì›í˜• ë³€í™˜ì„ ìœ„í•œ í›„ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def refine_verb(verb):\n",
    "    verb = re.sub(r\"ê³ $\", \"\", verb)  # \"ê³ \" ì œê±°\n",
    "    verb = re.sub(r\"ì–´$\", \"\", verb)  # \"ì–´\" ì œê±°\n",
    "    verb = re.sub(r\"ì•„$\", \"\", verb)  # \"ì•„\" ì œê±°\n",
    "    return verb + \"ë‹¤\"  # ì›í˜• ë³µì›\n",
    "\n",
    "# âœ… ë¶ˆí•„ìš”í•œ ì¡°ì‚¬ ë° \"+\" ë¬¸ì ì œê±° í•¨ìˆ˜\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence = re.sub(r\"\\+\", \"\", sentence)  # \"+\" ë¬¸ì ì™„ì „íˆ ì œê±°\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()  # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬\n",
    "    sentence = re.sub(r\"\\s*ì—ì„œ\\s*\", \" \", sentence)  # \"ì—ì„œ\" ì œê±°\n",
    "    sentence = re.sub(r\"\\s*ì—\\s*\", \" \", sentence)  # \"ì—\" ì œê±°\n",
    "    return sentence\n",
    "\n",
    "# âœ… ìˆ˜ì–´ì²´ ë³€í™˜ í•¨ìˆ˜ (ì£¼ì–´ ìœ ì§€ + ì¡°ì‚¬ ìë™ ì œê±°)\n",
    "def translate_to_sign_language(text):\n",
    "    doc = nlp(text)  # í˜•íƒœì†Œ ë¶„ì„ ìˆ˜í–‰\n",
    "    sign_sentence = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        words = []\n",
    "        verbs = []\n",
    "        removed_words = [\"PART\", \"PUNCT\", \"SCONJ\", \"AUX\", \"CCONJ\", \"ADP\", \"DET\"]  # ë¶ˆí•„ìš”í•œ ì¡°ì‚¬, ë¶€ì‚¬, ì ‘ì†ì‚¬ ì œê±°\n",
    "\n",
    "        for word in sentence.words:\n",
    "            if word.upos in removed_words:\n",
    "                continue\n",
    "\n",
    "            # âœ… ëª…ì‚¬(NOUN)ì—ì„œ ì¡°ì‚¬ ì œê±° (ì˜ˆ: \"ê³ ì–‘ì´ê°€\" â†’ \"ê³ ì–‘ì´\", \"ì±…ì„\" â†’ \"ì±…\")\n",
    "            if word.upos == \"PRON\" or word.upos == \"NOUN\":\n",
    "                words.append(remove_josa(word.text))  # ì¡°ì‚¬ ì œê±°ëœ ë‹¨ì–´ ì¶”ê°€\n",
    "                continue\n",
    "\n",
    "            # \"~ê³  ì‹¶ë‹¤\" ë³€í™˜: \"ì›í•˜ë‹¤\"ë¡œ ë³€ê²½\n",
    "            if word.text in [\"ì‹¶ë‹¤\"] and verbs:\n",
    "                verbs.append(\"ì›í•˜ë‹¤\")\n",
    "                continue\n",
    "\n",
    "            # \"ì•ˆ/ì•Šë‹¤\" ê°™ì€ ë¶€ì •ì–´ ì²˜ë¦¬\n",
    "            if word.text in [\"ì•ˆ\", \"ì•Šë‹¤\"]:\n",
    "                verbs.append(\"ì•„ë‹ˆë‹¤\")\n",
    "                continue\n",
    "\n",
    "            # âœ… ë™ì‚¬ ë³€í™˜ (ì˜ˆ: \"ë°°ìš°ê³ \" â†’ \"ë°°ìš°ë‹¤\")\n",
    "            if word.upos == \"VERB\":\n",
    "                lemma = refine_verb(word.lemma) if word.lemma else refine_verb(word.text)\n",
    "                verbs.append(lemma)  # ë™ì‚¬ ì›í˜• ë³€í™˜\n",
    "            else:\n",
    "                # âœ… ì¡°ì‚¬ ì œê±° (ì€, ëŠ”, ì´, ê°€, ë¥¼, ì„ ë“±ì„ ìë™ ì œê±°)\n",
    "                words.append(remove_josa(word.text))\n",
    "\n",
    "        # âœ… ìµœì¢… ë³€í™˜ëœ ë¬¸ì¥ ì¶”ê°€\n",
    "        words_sov = words + verbs  # ë™ì‚¬ë¥¼ ë¬¸ì¥ ëìœ¼ë¡œ ì´ë™\n",
    "        transformed_sentence = \" \".join(words_sov) + \".\"\n",
    "        cleaned_sentence = clean_up_sentence(transformed_sentence)  # ìµœì¢… ì •ë¦¬ëœ ë¬¸ì¥\n",
    "        \n",
    "        sign_sentence.append(cleaned_sentence)\n",
    "\n",
    "    return \" \".join(sign_sentence)\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ë¬¸ì¥\n",
    "test_sentences = [\n",
    "    \"ë‚˜ëŠ” ì§€ê¸ˆ ì§‘ì— ê°€ê³  ì‹¶ì–´.\",\n",
    "    \"ë‚´ì¼ ì˜í™” ë³´ê³  ì‹¶ì–´.\",\n",
    "    \"ì €ëŠ” ì§‘ì—ì„œ ë°¥ì„ ë¨¹ê³  ì‹¶ì–´ìš”.\",\n",
    "    \"ê³ ì–‘ì´ê°€ ì°½ë¬¸ ìœ„ì—ì„œ ìê³  ìˆì–´.\",\n",
    "    \"ë‚˜ëŠ” í•œêµ­ì–´ë¥¼ ë°°ìš°ê³  ì‹¶ì§€ ì•Šì•„.\",\n",
    "    \"ë‚˜ëŠ” ìƒˆë¡œìš´ í•¸ë“œí°ì„ ì‚¬ê³  ì‹¶ì–´.\",\n",
    "    \"ê·¸ëŠ” ì±…ì„ ì½ê³  ìˆì–´.\",\n",
    "    \"ë¬¸ë¬¸ì€ ë°¥ì„ ë¨¹ê³  ìˆì–´.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translated_text = translate_to_sign_language(sentence)\n",
    "    print(f\"ì›ë³¸ ë¬¸ì¥: {sentence}\")\n",
    "    print(f\"ìˆ˜ì–´ì²´ ë³€í™˜: {translated_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ë¬¸ì¥: ë¬¸ë¬¸ì€ ì§€ê¸ˆ ì—´ì‹¬íˆ ì‘ì—…ì„ í•˜ê³  ìˆì–´ìš”\n",
      "ìˆ˜ì–´ì²´ ë³€í™˜: ë¬¸ë¬¸ì€ ì§€ê¸ˆ ì—´ì‹¬íˆ ì‘ì—… í•˜ë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥\n",
    "test_sentences = [\n",
    "  \"ë¬¸ë¬¸ì€ ì§€ê¸ˆ ì—´ì‹¬íˆ ì‘ì—…ì„ í•˜ê³  ìˆì–´ìš”\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translated_text = translate_to_sign_language(sentence)\n",
    "    print(f\"ì›ë³¸ ë¬¸ì¥: {sentence}\")\n",
    "    print(f\"ìˆ˜ì–´ì²´ ë³€í™˜: {translated_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/usou/venv/superbad/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "ALSA lib pcm_dsnoop.c:567:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤ ìŒì„± ë…¹ìŒ ì‹œì‘... (2ì´ˆ ë™ì•ˆ)\n",
      "ğŸ”´ ë…¹ìŒ ì¢…ë£Œ\n",
      "\n",
      "ğŸ“ STT ë³€í™˜ ê²°ê³¼:  à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€à·€\n",
      "\n",
      "ğŸ“Š ê°ì • ë¶„ì„ ê²°ê³¼:\n",
      "  anger: 0.1190\n",
      "  disgust: 0.0434\n",
      "  fear: 0.2542\n",
      "  joy: 0.0237\n",
      "  neutral: 0.4818\n",
      "  sadness: 0.0639\n",
      "  surprise: 0.0141\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import whisper\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "# ë§ˆì´í¬ ì„¤ì •\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000  # Whisper ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ” ìƒ˜í”Œ ë ˆì´íŠ¸\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 2  # ë…¹ìŒ ì‹œê°„\n",
    "WAVE_OUTPUT_FILENAME = \"recorded_audio.wav\"\n",
    "\n",
    "# ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ (Hugging Face)\n",
    "emotion_model = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
    "\n",
    "def record_audio():\n",
    "    \"\"\"ë§ˆì´í¬ì—ì„œ ìŒì„±ì„ ë…¹ìŒí•˜ê³  íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "    print(\"ğŸ¤ ìŒì„± ë…¹ìŒ ì‹œì‘... ({}ì´ˆ ë™ì•ˆ)\".format(RECORD_SECONDS))\n",
    "    \n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"ğŸ”´ ë…¹ìŒ ì¢…ë£Œ\")\n",
    "\n",
    "    # ìŠ¤íŠ¸ë¦¼ ë‹«ê¸°\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # WAV íŒŒì¼ë¡œ ì €ì¥\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    return WAVE_OUTPUT_FILENAME\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    \"\"\"ë…¹ìŒëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ Whisperë¥¼ ì´ìš©í•´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "    model = whisper.load_model(\"small\")  # Whisper ëª¨ë¸ (tiny, base, small, medium, large ì„ íƒ ê°€ëŠ¥)\n",
    "    result = model.transcribe(file_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "def analyze_emotion(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ê°ì • ë¶„ì„ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ê°ì •ì„ ë¶„ì„\"\"\"\n",
    "    analysis = emotion_model(text)\n",
    "    return analysis\n",
    "\n",
    "# ì‹¤í–‰ íë¦„\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = record_audio()\n",
    "    transcribed_text = transcribe_audio(audio_file)\n",
    "    print(f\"\\nğŸ“ STT ë³€í™˜ ê²°ê³¼: {transcribed_text}\")\n",
    "\n",
    "    emotion_result = analyze_emotion(transcribed_text)\n",
    "    print(\"\\nğŸ“Š ê°ì • ë¶„ì„ ê²°ê³¼:\")\n",
    "    for emotion in emotion_result[0]:  # ê°€ì¥ ë†’ì€ ê°ì • ì„ íƒ\n",
    "        print(f\"  {emotion['label']}: {emotion['score']:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superbad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
