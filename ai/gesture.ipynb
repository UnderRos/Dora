{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 13:54:38.888257: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743051278.906376   21445 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743051278.911550   21445 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743051278.926766   21445 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743051278.926784   21445 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743051278.926785   21445 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743051278.926786   21445 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-27 13:54:38.931621: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743051284.719602   21445 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743051284.725529   21513 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1743051284.756507   21499 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743051284.779255   21505 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743051298.902967   21501 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boss (142, 100)\n",
      "boss (112, 30, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743051306.162529   21445 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4225 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/home/uk/dp_pj/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743051309.046334   21566 service.cc:152] XLA service 0x16800710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743051309.046353   21566 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2025-03-27 13:55:09.107440: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743051309.454529   21566 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "2025-03-27 13:55:11.110839: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 27/143\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2174 - loss: 29.1481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743051312.502589   21566 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.3369 - loss: 20.3450\n",
      "Epoch 1: val_acc improved from -inf to 0.57199, saving model to models/gesture.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - acc: 0.3375 - loss: 20.3076 - val_acc: 0.5720 - val_loss: 13.5900 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m139/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.4753 - loss: 113.4922\n",
      "Epoch 2: val_acc did not improve from 0.57199\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.4771 - loss: 112.0668 - val_acc: 0.5010 - val_loss: 18.7579 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m140/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.4442 - loss: 23.0914\n",
      "Epoch 3: val_acc did not improve from 0.57199\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.4452 - loss: 22.9659 - val_acc: 0.5621 - val_loss: 13.4540 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.4471 - loss: 23.7944\n",
      "Epoch 4: val_acc improved from 0.57199 to 0.68836, saving model to models/gesture.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - acc: 0.4484 - loss: 23.7116 - val_acc: 0.6884 - val_loss: 3.7038 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7526 - loss: 9.4851\n",
      "Epoch 5: val_acc improved from 0.68836 to 0.80868, saving model to models/gesture.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.7527 - loss: 9.4695 - val_acc: 0.8087 - val_loss: 2.6586 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m135/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7737 - loss: 4.5412\n",
      "Epoch 6: val_acc improved from 0.80868 to 0.83629, saving model to models/gesture.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.7747 - loss: 4.5215 - val_acc: 0.8363 - val_loss: 3.8625 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m134/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7062 - loss: 34.0988\n",
      "Epoch 7: val_acc did not improve from 0.83629\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.7018 - loss: 33.7754 - val_acc: 0.7357 - val_loss: 9.8977 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6605 - loss: 14.5617\n",
      "Epoch 8: val_acc did not improve from 0.83629\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.6608 - loss: 14.5190 - val_acc: 0.5858 - val_loss: 6.2575 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m135/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6469 - loss: 9.4157\n",
      "Epoch 9: val_acc improved from 0.83629 to 0.86193, saving model to models/gesture.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.6514 - loss: 9.3077 - val_acc: 0.8619 - val_loss: 2.1387 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8434 - loss: 2.0944\n",
      "Epoch 10: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8429 - loss: 2.1016 - val_acc: 0.7673 - val_loss: 2.2248 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m134/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.5840 - loss: 16.5925\n",
      "Epoch 11: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.5729 - loss: 17.4968 - val_acc: 0.5049 - val_loss: 18.8964 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m137/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.4618 - loss: 29.8234\n",
      "Epoch 12: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.4604 - loss: 29.8285 - val_acc: 0.0888 - val_loss: 306.5481 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m140/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.1419 - loss: 88.2167\n",
      "Epoch 13: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.1419 - loss: 86.8937 - val_acc: 0.1558 - val_loss: 2.2064 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m134/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.1547 - loss: 2.1795\n",
      "Epoch 14: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.1539 - loss: 2.1789 - val_acc: 0.1558 - val_loss: 2.1309 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m141/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.1436 - loss: 2.1153\n",
      "Epoch 15: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.1436 - loss: 2.1152 - val_acc: 0.1558 - val_loss: 2.0776 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m135/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2568 - loss: 2.0742\n",
      "Epoch 16: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2576 - loss: 2.0737 - val_acc: 0.2919 - val_loss: 2.0391 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m137/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2748 - loss: 2.0395\n",
      "Epoch 17: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2747 - loss: 2.0392 - val_acc: 0.2919 - val_loss: 2.0108 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m137/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2895 - loss: 2.0103\n",
      "Epoch 18: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2887 - loss: 2.0103 - val_acc: 0.2919 - val_loss: 1.9903 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m136/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2766 - loss: 1.9976\n",
      "Epoch 19: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2765 - loss: 1.9974 - val_acc: 0.2919 - val_loss: 1.9751 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m140/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2829 - loss: 1.9781\n",
      "Epoch 20: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2827 - loss: 1.9782 - val_acc: 0.2919 - val_loss: 1.9633 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m139/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2725 - loss: 1.9759\n",
      "Epoch 21: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2726 - loss: 1.9757 - val_acc: 0.2919 - val_loss: 1.9546 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m139/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2845 - loss: 1.9574\n",
      "Epoch 22: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2842 - loss: 1.9577 - val_acc: 0.2919 - val_loss: 1.9478 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m136/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2811 - loss: 1.9526\n",
      "Epoch 23: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2808 - loss: 1.9530 - val_acc: 0.2919 - val_loss: 1.9424 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m141/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2701 - loss: 1.9611\n",
      "Epoch 24: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2702 - loss: 1.9610 - val_acc: 0.2919 - val_loss: 1.9381 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m136/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2841 - loss: 1.9546\n",
      "Epoch 25: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2836 - loss: 1.9545 - val_acc: 0.2919 - val_loss: 1.9345 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.2870 - loss: 1.9366\n",
      "Epoch 26: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - acc: 0.2868 - loss: 1.9368 - val_acc: 0.2919 - val_loss: 1.9316 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m136/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.2793 - loss: 1.9371\n",
      "Epoch 27: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2790 - loss: 1.9378 - val_acc: 0.2919 - val_loss: 1.9292 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m134/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2797 - loss: 1.9474\n",
      "Epoch 28: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2793 - loss: 1.9474 - val_acc: 0.2919 - val_loss: 1.9272 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m137/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2740 - loss: 1.9587\n",
      "Epoch 29: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2741 - loss: 1.9581 - val_acc: 0.2919 - val_loss: 1.9252 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m141/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2847 - loss: 1.9340\n",
      "Epoch 30: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2845 - loss: 1.9342 - val_acc: 0.2919 - val_loss: 1.9238 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m135/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2743 - loss: 1.9468\n",
      "Epoch 31: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2743 - loss: 1.9467 - val_acc: 0.2919 - val_loss: 1.9226 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m137/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2691 - loss: 1.9507\n",
      "Epoch 32: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2694 - loss: 1.9503 - val_acc: 0.2919 - val_loss: 1.9213 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2754 - loss: 1.9461\n",
      "Epoch 33: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2754 - loss: 1.9460 - val_acc: 0.2919 - val_loss: 1.9203 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2856 - loss: 1.9266\n",
      "Epoch 34: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2855 - loss: 1.9267 - val_acc: 0.2919 - val_loss: 1.9195 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m136/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2739 - loss: 1.9437\n",
      "Epoch 35: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2739 - loss: 1.9437 - val_acc: 0.2919 - val_loss: 1.9188 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m141/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2854 - loss: 1.9332\n",
      "Epoch 36: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2852 - loss: 1.9334 - val_acc: 0.2919 - val_loss: 1.9182 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m134/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2734 - loss: 1.9412\n",
      "Epoch 37: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2734 - loss: 1.9412 - val_acc: 0.2919 - val_loss: 1.9175 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2780 - loss: 1.9409\n",
      "Epoch 38: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2779 - loss: 1.9409 - val_acc: 0.2919 - val_loss: 1.9171 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m135/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2803 - loss: 1.9516\n",
      "Epoch 39: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2800 - loss: 1.9510 - val_acc: 0.2919 - val_loss: 1.9165 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m135/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2784 - loss: 1.9416\n",
      "Epoch 40: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2782 - loss: 1.9416 - val_acc: 0.2919 - val_loss: 1.9161 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m141/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2835 - loss: 1.9344\n",
      "Epoch 41: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2833 - loss: 1.9345 - val_acc: 0.2919 - val_loss: 1.9158 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m135/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2761 - loss: 1.9460\n",
      "Epoch 42: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2760 - loss: 1.9457 - val_acc: 0.2919 - val_loss: 1.9152 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m139/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2789 - loss: 1.9341\n",
      "Epoch 43: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2788 - loss: 1.9343 - val_acc: 0.2919 - val_loss: 1.9148 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m133/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2863 - loss: 1.9303\n",
      "Epoch 44: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2855 - loss: 1.9309 - val_acc: 0.2919 - val_loss: 1.9147 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2870 - loss: 1.9256\n",
      "Epoch 45: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2869 - loss: 1.9257 - val_acc: 0.2919 - val_loss: 1.9145 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m139/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.2837 - loss: 1.9254\n",
      "Epoch 46: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - acc: 0.2833 - loss: 1.9259 - val_acc: 0.2919 - val_loss: 1.9141 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m138/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2736 - loss: 1.9385\n",
      "Epoch 47: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2737 - loss: 1.9385 - val_acc: 0.2919 - val_loss: 1.9140 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m134/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2741 - loss: 1.9413\n",
      "Epoch 48: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2742 - loss: 1.9411 - val_acc: 0.2919 - val_loss: 1.9138 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m138/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2629 - loss: 1.9440\n",
      "Epoch 49: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2634 - loss: 1.9438 - val_acc: 0.2919 - val_loss: 1.9137 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m137/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2826 - loss: 1.9388\n",
      "Epoch 50: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2822 - loss: 1.9389 - val_acc: 0.2919 - val_loss: 1.9134 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m137/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2628 - loss: 1.9576\n",
      "Epoch 51: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2634 - loss: 1.9567 - val_acc: 0.2919 - val_loss: 1.9132 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m141/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2746 - loss: 1.9350\n",
      "Epoch 52: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2746 - loss: 1.9351 - val_acc: 0.2919 - val_loss: 1.9130 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m141/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2683 - loss: 1.9456\n",
      "Epoch 53: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2684 - loss: 1.9455 - val_acc: 0.2919 - val_loss: 1.9130 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m137/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2806 - loss: 1.9232\n",
      "Epoch 54: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2804 - loss: 1.9240 - val_acc: 0.2919 - val_loss: 1.9127 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m141/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2694 - loss: 1.9455\n",
      "Epoch 55: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2695 - loss: 1.9454 - val_acc: 0.2919 - val_loss: 1.9128 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m136/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2824 - loss: 1.9301\n",
      "Epoch 56: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2820 - loss: 1.9306 - val_acc: 0.2919 - val_loss: 1.9126 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m135/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2720 - loss: 1.9382\n",
      "Epoch 57: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2722 - loss: 1.9384 - val_acc: 0.2919 - val_loss: 1.9125 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m138/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2687 - loss: 1.9399\n",
      "Epoch 58: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2690 - loss: 1.9399 - val_acc: 0.2919 - val_loss: 1.9126 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m139/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2762 - loss: 1.9353\n",
      "Epoch 59: val_acc did not improve from 0.86193\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.2762 - loss: 1.9354 - val_acc: 0.2919 - val_loss: 1.9125 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m141/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2693 - loss: 1.9456\n",
      "Epoch 60: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2694 - loss: 1.9455 - val_acc: 0.2919 - val_loss: 1.9124 - learning_rate: 5.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2837 - loss: 1.9245\n",
      "Epoch 61: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2836 - loss: 1.9246 - val_acc: 0.2919 - val_loss: 1.9124 - learning_rate: 5.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2786 - loss: 1.9393\n",
      "Epoch 62: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2786 - loss: 1.9393 - val_acc: 0.2919 - val_loss: 1.9123 - learning_rate: 5.0000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2792 - loss: 1.9343\n",
      "Epoch 63: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2791 - loss: 1.9343 - val_acc: 0.2919 - val_loss: 1.9124 - learning_rate: 5.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m136/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2623 - loss: 1.9503\n",
      "Epoch 64: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2629 - loss: 1.9498 - val_acc: 0.2919 - val_loss: 1.9123 - learning_rate: 5.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m140/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2718 - loss: 1.9403\n",
      "Epoch 65: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2719 - loss: 1.9403 - val_acc: 0.2919 - val_loss: 1.9122 - learning_rate: 5.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m138/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2681 - loss: 1.9383\n",
      "Epoch 66: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2684 - loss: 1.9383 - val_acc: 0.2919 - val_loss: 1.9122 - learning_rate: 5.0000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m134/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.2734 - loss: 1.9440\n",
      "Epoch 67: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - acc: 0.2735 - loss: 1.9437 - val_acc: 0.2919 - val_loss: 1.9122 - learning_rate: 5.0000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m138/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2628 - loss: 1.9518\n",
      "Epoch 68: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2633 - loss: 1.9513 - val_acc: 0.2919 - val_loss: 1.9121 - learning_rate: 5.0000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2803 - loss: 1.9394\n",
      "Epoch 69: val_acc did not improve from 0.86193\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.2802 - loss: 1.9394 - val_acc: 0.2919 - val_loss: 1.9121 - learning_rate: 5.0000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m 89/143\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.2784 - loss: 1.9296"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 148\u001b[39m\n\u001b[32m    145\u001b[39m data_path = \u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    146\u001b[39m model_path = \u001b[33m'\u001b[39m\u001b[33mmodels/gesture.h5\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[43mcollect_and_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgesture_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 128\u001b[39m, in \u001b[36mcollect_and_train\u001b[39m\u001b[34m(gesture_name, data_path, model_path, secs_for_action)\u001b[39m\n\u001b[32m    120\u001b[39m model = Sequential([\n\u001b[32m    121\u001b[39m     LSTM(\u001b[32m64\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m, input_shape=x_train.shape[\u001b[32m1\u001b[39m:\u001b[32m3\u001b[39m]),\n\u001b[32m    122\u001b[39m     Dense(\u001b[32m32\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    123\u001b[39m     Dense(\u001b[38;5;28mlen\u001b[39m(actions), activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    124\u001b[39m ])\n\u001b[32m    126\u001b[39m model.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33macc\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_acc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_acc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# 모델 학습 후 레이블 저장\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mmodels/gesture_labels.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dp_pj/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time, os\n",
    "import json\n",
    "from tensorflow.keras.utils import to_categorical   \n",
    "from sklearn.model_selection import train_test_split    \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense         \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau       \n",
    "def collect_and_train(gesture_name, data_path, model_path, secs_for_action=15):\n",
    "    \"\"\"\n",
    "    사용자로부터 제스처 데이터를 수집하고 기존 제스처 파일들과 함께 모델을 학습합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    actions = []  \n",
    "    seq_length = 30    \n",
    "\n",
    "    # MediaPipe hands model\n",
    "    mp_hands = mp.solutions.hands      \n",
    "    mp_drawing = mp.solutions.drawing_utils       \n",
    "    hands = mp_hands.Hands(                     \n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    created_time = int(time.time())             \n",
    "    os.makedirs(data_path, exist_ok=True)         \n",
    "\n",
    "    # 새로운 데이터 수집\n",
    "    while cap.isOpened():\n",
    "        for idx, action in enumerate([gesture_name]): \n",
    "            data = []\n",
    "\n",
    "            ret, img = cap.read()                \n",
    "            img = cv2.flip(img, 1)          \n",
    "\n",
    "            cv2.putText(img, f'Collecting {action.upper()} action...', org=(10, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "            cv2.imshow('img', img)       \n",
    "            cv2.waitKey(3000)\n",
    "\n",
    "            start_time = time.time()      \n",
    "\n",
    "            while time.time() - start_time < secs_for_action:   \n",
    "                ret, img = cap.read()                               \n",
    "                img = cv2.flip(img, 1)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)        \n",
    "                result = hands.process(img)                       \n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                if result.multi_hand_landmarks is not None:      \n",
    "                    for res in result.multi_hand_landmarks:       \n",
    "                        joint = np.zeros((21, 4))               \n",
    "                        for j, lm in enumerate(res.landmark):\n",
    "                            joint[j] = [lm.x, lm.y, lm.z, lm.visibility]    \n",
    "\n",
    "                        v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]      \n",
    "                        v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]    \n",
    "                        v = v2 - v1      \n",
    "                        v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]   \n",
    "\n",
    "                        angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                            v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],\n",
    "                            v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]))         \n",
    "\n",
    "                        angle = np.degrees(angle)         \n",
    "\n",
    "                        angle_label = np.array([angle], dtype=np.float32)           \n",
    "                        angle_label = np.append(angle_label, idx)                  \n",
    "\n",
    "                        d = np.concatenate([joint.flatten(), angle_label])          \n",
    "                        data.append(d)                                              \n",
    "                        mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS)      \n",
    "\n",
    "                cv2.imshow('img', img)              \n",
    "                if cv2.waitKey(1) == ord('q'):\n",
    "                    return\n",
    "\n",
    "            data = np.array(data)\n",
    "            print(action, data.shape)\n",
    "            np.save(os.path.join(data_path, f'raw_{action}_{created_time}'), data)\n",
    "\n",
    "            full_seq_data = []\n",
    "            for seq in range(len(data) - seq_length):\n",
    "                full_seq_data.append(data[seq:seq + seq_length])\n",
    "\n",
    "            full_seq_data = np.array(full_seq_data)\n",
    "            print(action, full_seq_data.shape)\n",
    "            np.save(os.path.join(data_path, f'seq_{action}_{created_time}'), full_seq_data)\n",
    "        break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # 기존 데이터 로드 및 병합\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.startswith('seq_') and filename.endswith('.npy'):\n",
    "            action_name = filename.split('_')[1]\n",
    "            if action_name not in actions:\n",
    "                actions.append(action_name)\n",
    "\n",
    "            action_index = actions.index(action_name)\n",
    "            data = np.load(os.path.join(data_path, filename))\n",
    "            x_data.extend(data[:, :, :-1].tolist())\n",
    "            y_data.extend([action_index] * len(data))\n",
    "\n",
    "    x_data = np.array(x_data)\n",
    "    y_data = np.array(y_data)\n",
    "    y_data = to_categorical(y_data, num_classes=len(actions))\n",
    "\n",
    "    x_data = x_data.astype(np.float32)\n",
    "    y_data = y_data.astype(np.float32)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=43)\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(len(actions), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=200,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(model_path, monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "            ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 모델 학습 후 레이블 저장\n",
    "    with open('ai/training/data/gesture_labels.json', 'w') as f:\n",
    "        json.dump(actions, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gesture_name = input(\"학습할 제스처 이름을 입력하세요: \")\n",
    "    data_path = 'ai/training/data/gesture_seed_data'\n",
    "    model_path = 'ai/models/gesture_model.h5'\n",
    "\n",
    "    collect_and_train(gesture_name, data_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Model file not found at models/model.h5\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "def recognize_gesture(model_path):\n",
    "    \"\"\"\n",
    "    실시간으로 제스처를 인식하고 화면에 표시합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    seq_length = 30\n",
    "\n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return\n",
    "\n",
    "    # 모델의 레이블(제스처 이름) 가져오기\n",
    "    try:\n",
    "        with open('models/gesture_labels.json', 'r') as f:\n",
    "            actions = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: gesture_labels.json file not found.\")\n",
    "        return\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    hands = mp_hands.Hands(\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    seq = []\n",
    "    action_seq = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img = cv2.flip(img, 1)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(img_rgb)\n",
    "        img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks in result.multi_hand_landmarks:\n",
    "                joint = np.zeros((21, 4))\n",
    "                for j, landmark in enumerate(hand_landmarks.landmark):\n",
    "                    joint[j] = [landmark.x, landmark.y, landmark.z, landmark.visibility]\n",
    "\n",
    "                v1 = joint[[0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 0, 13, 14, 15, 0, 17, 18, 19], :3]\n",
    "                v2 = joint[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], :3]\n",
    "                v = v2 - v1\n",
    "                v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "                angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                    v[[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18], :],\n",
    "                    v[[1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19], :]))\n",
    "                angle = np.degrees(angle)\n",
    "\n",
    "                d = np.concatenate([joint.flatten(), angle])\n",
    "                seq.append(d)\n",
    "\n",
    "                mp_drawing.draw_landmarks(img_bgr, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if len(seq) < seq_length:\n",
    "                    continue\n",
    "\n",
    "                input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "                y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "                if y_pred.ndim == 0:\n",
    "                    print(\"No prediction result.\")\n",
    "                    continue\n",
    "\n",
    "                i_pred = int(np.argmax(y_pred))\n",
    "                conf = y_pred[i_pred]\n",
    "\n",
    "                if conf < 0.9:\n",
    "                    continue\n",
    "\n",
    "                predicted_action = actions[i_pred]\n",
    "                action_seq.append(predicted_action)\n",
    "\n",
    "                if len(action_seq) < 3:\n",
    "                    continue\n",
    "\n",
    "                this_action = '?'\n",
    "                if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                    this_action = predicted_action\n",
    "\n",
    "                # 제스처 이름 및 신뢰도를 화면에 표시\n",
    "                text = f'{this_action.upper()} ({conf:.2f})'\n",
    "                cv2.putText(img_bgr, text, org=(10, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "        cv2.imshow('img', img_bgr)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = 'models/gesture_model.h5'\n",
    "    recognize_gesture(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp_pj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
